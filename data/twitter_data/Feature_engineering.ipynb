{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc70945",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Create a Dataframe for model input:\n",
    "- Feature creation\n",
    "- Daily aggregation of features\n",
    "- Vizualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762bcff",
   "metadata": {},
   "source": [
    "# Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc331a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\paull\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\paull\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Standard Libraries ---\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# --- Data Science ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# --- NLP ---\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from langdetect import detect, DetectorFactory, LangDetectException\n",
    "\n",
    "# --- Transformers ---\n",
    "import torch\n",
    "from torch.nn.functional import sigmoid\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification\n",
    ")\n",
    "from scipy.special import softmax\n",
    "\n",
    "# --- Utils ---\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Setup ---\n",
    "tqdm.pandas()\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6976aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1465e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paull\\AppData\\Local\\Temp\\ipykernel_16836\\4162121391.py:1: DtypeWarning: Columns (11,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  musk_twitter_data_all = pd.read_csv(os.path.join('processed', 'musk_twitter_data_all.csv'),parse_dates=[\"createdAt\"])\n",
      "C:\\Users\\paull\\AppData\\Local\\Temp\\ipykernel_16836\\4162121391.py:2: DtypeWarning: Columns (11,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  musk_twitter_data_nlp = pd.read_csv(os.path.join('processed', 'musk_twitter_data_nlp.csv'),parse_dates=[\"createdAt\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPTweets: (4111, 28) AllTweets: (6327, 25)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4111 entries, 0 to 4110\n",
      "Data columns (total 28 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   id                        4111 non-null   int64              \n",
      " 1   url                       4111 non-null   object             \n",
      " 2   twitterUrl                4111 non-null   object             \n",
      " 3   fullText                  4111 non-null   object             \n",
      " 4   retweetCount              4111 non-null   float64            \n",
      " 5   replyCount                4111 non-null   float64            \n",
      " 6   likeCount                 4111 non-null   float64            \n",
      " 7   quoteCount                4111 non-null   float64            \n",
      " 8   viewCount                 4111 non-null   float64            \n",
      " 9   createdAt                 4111 non-null   datetime64[ns, UTC]\n",
      " 10  bookmarkCount             4111 non-null   float64            \n",
      " 11  isReply                   4111 non-null   object             \n",
      " 12  inReplyToId               2058 non-null   float64            \n",
      " 13  conversationId            4111 non-null   float64            \n",
      " 14  inReplyToUserId           2068 non-null   float64            \n",
      " 15  inReplyToUsername         2068 non-null   object             \n",
      " 16  isPinned                  4111 non-null   object             \n",
      " 17  isRetweet                 4111 non-null   object             \n",
      " 18  isQuote                   4111 non-null   object             \n",
      " 19  isConversationControlled  4111 non-null   object             \n",
      " 20  possiblySensitive         4111 non-null   object             \n",
      " 21  quoteId                   1751 non-null   float64            \n",
      " 22  quote                     1738 non-null   object             \n",
      " 23  retweet                   0 non-null      float64            \n",
      " 24  date                      4111 non-null   object             \n",
      " 25  language                  4111 non-null   object             \n",
      " 26  text_raw                  4111 non-null   object             \n",
      " 27  text_lemmatized           4111 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(11), int64(1), object(15)\n",
      "memory usage: 899.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6327 entries, 0 to 6326\n",
      "Data columns (total 25 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   id                        6327 non-null   int64              \n",
      " 1   url                       6327 non-null   object             \n",
      " 2   twitterUrl                6327 non-null   object             \n",
      " 3   fullText                  6327 non-null   object             \n",
      " 4   retweetCount              6327 non-null   float64            \n",
      " 5   replyCount                6327 non-null   float64            \n",
      " 6   likeCount                 6327 non-null   float64            \n",
      " 7   quoteCount                6327 non-null   float64            \n",
      " 8   viewCount                 6327 non-null   float64            \n",
      " 9   createdAt                 6327 non-null   datetime64[ns, UTC]\n",
      " 10  bookmarkCount             6327 non-null   float64            \n",
      " 11  isReply                   6327 non-null   object             \n",
      " 12  inReplyToId               2834 non-null   float64            \n",
      " 13  conversationId            6327 non-null   float64            \n",
      " 14  inReplyToUserId           2844 non-null   float64            \n",
      " 15  inReplyToUsername         2844 non-null   object             \n",
      " 16  isPinned                  6327 non-null   object             \n",
      " 17  isRetweet                 6327 non-null   object             \n",
      " 18  isQuote                   6327 non-null   object             \n",
      " 19  isConversationControlled  6327 non-null   object             \n",
      " 20  possiblySensitive         6327 non-null   object             \n",
      " 21  quoteId                   2655 non-null   float64            \n",
      " 22  quote                     2546 non-null   object             \n",
      " 23  retweet                   454 non-null    object             \n",
      " 24  date                      6327 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), float64(10), int64(1), object(13)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "musk_twitter_data_all = pd.read_csv(os.path.join('cleaned', 'musk_twitter_data_all.csv'),parse_dates=[\"createdAt\"])\n",
    "musk_twitter_data_nlp = pd.read_csv(os.path.join('cleaned', 'musk_twitter_data_nlp.csv'),parse_dates=[\"createdAt\"])\n",
    "\n",
    "for df in (musk_twitter_data_all, musk_twitter_data_nlp):\n",
    "    df[\"isRetweet\"] = df[\"isRetweet\"].astype(str).str.lower()\n",
    "    df[\"possiblySensitive\"] = df[\"possiblySensitive\"].astype(str).str.lower()\n",
    "    df[\"fullText\"] = df[\"fullText\"].astype(str)\n",
    "\n",
    "musk_twitter_data_nlp[\"text_raw\"] = musk_twitter_data_nlp[\"text_raw\"].astype(str)\n",
    "musk_twitter_data_nlp[\"text_lemmatized\"] = musk_twitter_data_nlp[\"text_lemmatized\"].astype(str)\n",
    "\n",
    "for df in (musk_twitter_data_all, musk_twitter_data_nlp):\n",
    "    df[\"date\"] = df[\"createdAt\"].dt.date\n",
    "\n",
    "if TEST:\n",
    "    start_date = pd.to_datetime(\"2025-01-01\").date()\n",
    "else:\n",
    "    start_date = pd.to_datetime(\"2015-01-01\").date()\n",
    "\n",
    "end_date = musk_twitter_data_all[\"date\"].max()\n",
    "\n",
    "mask_all = (musk_twitter_data_all[\"date\"] >= start_date) & (musk_twitter_data_all[\"date\"] <= end_date)\n",
    "musk_twitter_data_all = musk_twitter_data_all.loc[mask_all].reset_index(drop=True)\n",
    "\n",
    "mask_nlp = (musk_twitter_data_nlp[\"date\"] >= start_date) & (musk_twitter_data_nlp[\"date\"] <= end_date)\n",
    "musk_twitter_data_nlp = musk_twitter_data_nlp.loc[mask_nlp].reset_index(drop=True)\n",
    "\n",
    "final_daily_df = pd.DataFrame({\n",
    "    'date': pd.date_range(start=start_date, end=end_date)\n",
    "})\n",
    "final_daily_df[\"date\"] = final_daily_df[\"date\"].dt.date \n",
    "\n",
    "print(\"NLPTweets:\", musk_twitter_data_nlp.shape, \"AllTweets:\", musk_twitter_data_all.shape)\n",
    "musk_twitter_data_nlp.info()\n",
    "musk_twitter_data_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd779d",
   "metadata": {},
   "source": [
    "# Tweet activity\n",
    "New features:\n",
    "- Number of tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ad6dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts_daily = (\n",
    "    musk_twitter_data_all\n",
    "    .groupby(\"date\")\n",
    "    .size()\n",
    "    .reset_index(name=\"tweet_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9903ea",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "New Features:\n",
    "- Poitve, Neutral ans Negative percentage of posts\n",
    "- Polarization: Tweets with pos/neg > 0,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ec5ff4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd923433cb9648bcb8865f34185a4337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Tweet sentiment analysis\n",
    "# Model: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(text):\n",
    "    return text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "def get_sentiment_probs(text):\n",
    "    text = preprocess(text)\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    probs = softmax(output.logits.cpu().numpy()[0])\n",
    "    return {\n",
    "        \"sentiment\": ['negative', 'neutral', 'positive'][probs.argmax()],\n",
    "        \"neg\": probs[0],\n",
    "        \"neu\": probs[1],\n",
    "        \"pos\": probs[2],\n",
    "    }\n",
    "\n",
    "def polarized_label(row):\n",
    "    return \"polarized\" if max(row[\"pos\"], row[\"neg\"]) > 0.6 else \"not_polarized\"\n",
    "\n",
    "results = musk_twitter_data_nlp[\"text_raw\"].progress_apply(get_sentiment_probs).apply(pd.Series)\n",
    "musk_twitter_data_nlp = pd.concat([musk_twitter_data_nlp, results], axis=1)\n",
    "musk_twitter_data_nlp[\"sentiment_polarity\"] = musk_twitter_data_nlp.apply(polarized_label, axis=1)\n",
    "\n",
    "# Daily aggregation\n",
    "sentiment_avg = (musk_twitter_data_nlp.groupby(\"date\")[[\"neg\", \"neu\", \"pos\"]].mean().reset_index())\n",
    "nlp_counts = (musk_twitter_data_nlp.groupby(\"date\").size().reset_index(name=\"nlp_tweet_count\"))\n",
    "polarization = (musk_twitter_data_nlp.groupby(\"date\")[\"sentiment_polarity\"].value_counts(normalize=True).unstack(fill_value=0).reset_index())\n",
    "\n",
    "# 4. Merge zu tagesbasiertem Datensatz\n",
    "sentiment_daily = sentiment_avg.merge(nlp_counts, on=\"date\", how=\"left\")\n",
    "sentiment_daily = sentiment_daily.merge(polarization, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb69ab",
   "metadata": {},
   "source": [
    "# Emotions & Personality\n",
    "\n",
    "New Features:\n",
    "- Ekman Emotions: anger, disgust, fear, joy, neutral, sadness, surprise\n",
    "- Big 5 personality traits: Extroversion, Neuroticism, Agreeableness, Conscientiousness, Openness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc900597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating emotion probabilities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f0ba07e17841bd986a2c563fe99bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily emotions...\n"
     ]
    }
   ],
   "source": [
    "# Ekman Emotionen\n",
    "# Model: https://huggingface.co/j-hartmann/emotion-english-distilroberta-base\n",
    "model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "# Helper that returns a dict of probabilities\n",
    "def get_emotions(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**tokens).logits\n",
    "    probs = softmax(logits.numpy()[0])\n",
    "    return dict(zip(emotion_labels, probs))\n",
    "\n",
    "# Apply to every tweet\n",
    "print(\"Calculating emotion probabilities...\")\n",
    "emotion_probs = musk_twitter_data_nlp['text_raw'].progress_apply(get_emotions).apply(pd.Series)\n",
    "\n",
    "# Append those new columns back onto your original DF\n",
    "musk_twitter_data_nlp = pd.concat([musk_twitter_data_nlp.reset_index(drop=True), emotion_probs],axis=1)\n",
    "\n",
    "# Aggregate by day (mean probability for each emotion)\n",
    "print(\"Aggregating daily emotions...\")\n",
    "emotion_daily = (musk_twitter_data_nlp.groupby('date')[emotion_labels].mean().reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ccdbb0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating personality traits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32a4883091d49a199245ffb258b6672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily personality...\n"
     ]
    }
   ],
   "source": [
    "# Big Five Personality Traits\n",
    "# Model: https://huggingface.co/Minej/bert-base-personality\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Minej/bert-base-personality\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"Minej/bert-base-personality\")\n",
    "\n",
    "personality_labels = ['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']\n",
    "\n",
    "def get_personality(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = sigmoid(outputs.logits).squeeze().numpy()\n",
    "    return dict(zip(personality_labels, probs))\n",
    "\n",
    "# Apply to every tweet\n",
    "print(\"Calculating personality traits...\")\n",
    "personality_probs = musk_twitter_data_nlp['text_raw'].progress_apply(get_personality).apply(pd.Series)\n",
    "\n",
    "# Append those new columns back onto your original DF\n",
    "musk_twitter_data_nlp = pd.concat([musk_twitter_data_nlp.reset_index(drop=True), personality_probs], axis=1)\n",
    "\n",
    "# Aggregate by day (mean probability for each emotion)\n",
    "print(\"Aggregating daily personality...\")\n",
    "personality_daily = (musk_twitter_data_nlp.groupby('date')[personality_labels].mean().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5fc4b",
   "metadata": {},
   "source": [
    "# Topic and word counts\n",
    "\n",
    "New Features: \n",
    "- Daily Word counts\n",
    "    - Rationale of Definition of words:\n",
    "        - Company/ticker terms (e.g. tesla, tsla, spacex) capture direct references to publicly traded entities.\n",
    "        - Product names (e.g. model, cybertruck, starship) often precede news that can move stock prices.\n",
    "        - Crypto tokens (e.g. bitcoin, dogecoin, ethereum, crypto) map to Musk-driven volatility in the digital-asset markets\n",
    "        - Financial keywords (e.g. stock, market, price, profit, loss, revenue) directly signal earnings or valuation discussions.\n",
    "        - Macro terms (e.g. inflation, interest) reflect broader economic commentary that can sway sentiment.\n",
    "        - Action verbs (buy, sell) often presage trading intent or recommendations.\n",
    "- Topics of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9902eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words\n",
    "def tokenize(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|@\\S+|[^a-z\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "# All words\n",
    "all_tokens = musk_twitter_data_nlp[\"text_lemmatized\"].dropna().apply(tokenize)\n",
    "flat_tokens = [token for sublist in all_tokens for token in sublist]\n",
    "word_counts = Counter(flat_tokens)\n",
    "word_counts = pd.DataFrame(word_counts.items(), columns=[\"word\", \"count\"]).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Top 20 words daily\n",
    "top20 = [\n",
    "    'tesla','tsla','stock','market','price','profit','loss','revenue',\n",
    "    'inflation','interest','bitcoin','dogecoin','crypto','ethereum',\n",
    "    'spacex','model','cybertruck','starship','buy','sell'\n",
    "]\n",
    "\n",
    "top_word_df = musk_twitter_data_nlp.dropna(subset=['text_lemmatized']).copy()\n",
    "top_word_df['tokens'] = top_word_df['text_lemmatized'].apply(tokenize)\n",
    "top_word_df = top_word_df.explode('tokens')\n",
    "top_word_df = top_word_df[top_word_df['tokens'].isin(top20)].copy()\n",
    "\n",
    "daily_word_counts = (\n",
    "    top_word_df\n",
    "    .groupby(['date','tokens'])\n",
    "    .size()               \n",
    "    .unstack(fill_value=0)   \n",
    "    .reindex(columns=top20,    \n",
    "               fill_value=0)\n",
    "    .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4c03c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating topic probabilities...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862b959bc18f46949c60e8f6e26345bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating daily topics...\n"
     ]
    }
   ],
   "source": [
    "# Topics\n",
    "model_name = \"cardiffnlp/tweet-topic-21-multi\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "topic_labels = [\n",
    "    \"arts_&_culture\", \"business_&_entrepreneurs\", \"celebrity_&_pop_culture\",\n",
    "    \"diaries_&_daily_life\", \"family\", \"fashion_&_style\", \"film_tv_&_video\",\n",
    "    \"fitness_&_health\", \"food_&_dining\", \"gaming\", \"learning_&_educational\",\n",
    "    \"music\", \"news_&_social_concern\", \"other_hobbies\", \"relationships\",\n",
    "    \"science_&_technology\", \"sports\", \"travel_&_adventure\", \"youth_&_student_life\"\n",
    "]\n",
    "\n",
    "def get_topics(text):\n",
    "    tokens = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    probs = softmax(output.logits.numpy()[0])\n",
    "    return dict(zip(topic_labels, probs))\n",
    "\n",
    "# Apply to every tweet\n",
    "print(\"Calculating topic probabilities...\")\n",
    "topic_scores = musk_twitter_data_nlp['text_lemmatized'].progress_apply(get_topics).apply(pd.Series)\n",
    "\n",
    "# Append those new columns back onto your original DF\n",
    "musk_twitter_data_nlp = pd.concat([musk_twitter_data_nlp.reset_index(drop=True), topic_scores], axis=1)\n",
    "\n",
    "# Aggregate by day (mean probability for each topic)\n",
    "print(\"Aggregating daily topics...\")\n",
    "topics_daily = (musk_twitter_data_nlp.groupby('date')[topic_labels].mean().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c39b38",
   "metadata": {},
   "source": [
    "# Merge and Create final df\n",
    "Merge the daily dfs in one dataframe and create csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0364eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103 entries, 0 to 102\n",
      "Data columns (total 59 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   date                      103 non-null    object \n",
      " 1   tweet_count               103 non-null    int64  \n",
      " 2   neg                       103 non-null    float32\n",
      " 3   neu                       103 non-null    float32\n",
      " 4   pos                       103 non-null    float32\n",
      " 5   nlp_tweet_count           103 non-null    int64  \n",
      " 6   not_polarized             103 non-null    float64\n",
      " 7   polarized                 103 non-null    float64\n",
      " 8   anger                     103 non-null    float32\n",
      " 9   disgust                   103 non-null    float32\n",
      " 10  fear                      103 non-null    float32\n",
      " 11  joy                       103 non-null    float32\n",
      " 12  neutral                   103 non-null    float32\n",
      " 13  sadness                   103 non-null    float32\n",
      " 14  surprise                  103 non-null    float32\n",
      " 15  Extroversion              103 non-null    float32\n",
      " 16  Neuroticism               103 non-null    float32\n",
      " 17  Agreeableness             103 non-null    float32\n",
      " 18  Conscientiousness         103 non-null    float32\n",
      " 19  Openness                  103 non-null    float32\n",
      " 20  tesla                     68 non-null     float64\n",
      " 21  tsla                      68 non-null     float64\n",
      " 22  stock                     68 non-null     float64\n",
      " 23  market                    68 non-null     float64\n",
      " 24  price                     68 non-null     float64\n",
      " 25  profit                    68 non-null     float64\n",
      " 26  loss                      68 non-null     float64\n",
      " 27  revenue                   68 non-null     float64\n",
      " 28  inflation                 68 non-null     float64\n",
      " 29  interest                  68 non-null     float64\n",
      " 30  bitcoin                   68 non-null     float64\n",
      " 31  dogecoin                  68 non-null     float64\n",
      " 32  crypto                    68 non-null     float64\n",
      " 33  ethereum                  68 non-null     float64\n",
      " 34  spacex                    68 non-null     float64\n",
      " 35  model                     68 non-null     float64\n",
      " 36  cybertruck                68 non-null     float64\n",
      " 37  starship                  68 non-null     float64\n",
      " 38  buy                       68 non-null     float64\n",
      " 39  sell                      68 non-null     float64\n",
      " 40  arts_&_culture            103 non-null    float32\n",
      " 41  business_&_entrepreneurs  103 non-null    float32\n",
      " 42  celebrity_&_pop_culture   103 non-null    float32\n",
      " 43  diaries_&_daily_life      103 non-null    float32\n",
      " 44  family                    103 non-null    float32\n",
      " 45  fashion_&_style           103 non-null    float32\n",
      " 46  film_tv_&_video           103 non-null    float32\n",
      " 47  fitness_&_health          103 non-null    float32\n",
      " 48  food_&_dining             103 non-null    float32\n",
      " 49  gaming                    103 non-null    float32\n",
      " 50  learning_&_educational    103 non-null    float32\n",
      " 51  music                     103 non-null    float32\n",
      " 52  news_&_social_concern     103 non-null    float32\n",
      " 53  other_hobbies             103 non-null    float32\n",
      " 54  relationships             103 non-null    float32\n",
      " 55  science_&_technology      103 non-null    float32\n",
      " 56  sports                    103 non-null    float32\n",
      " 57  travel_&_adventure        103 non-null    float32\n",
      " 58  youth_&_student_life      103 non-null    float32\n",
      "dtypes: float32(34), float64(22), int64(2), object(1)\n",
      "memory usage: 33.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>nlp_tweet_count</th>\n",
       "      <th>not_polarized</th>\n",
       "      <th>polarized</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>...</th>\n",
       "      <th>gaming</th>\n",
       "      <th>learning_&amp;_educational</th>\n",
       "      <th>music</th>\n",
       "      <th>news_&amp;_social_concern</th>\n",
       "      <th>other_hobbies</th>\n",
       "      <th>relationships</th>\n",
       "      <th>science_&amp;_technology</th>\n",
       "      <th>sports</th>\n",
       "      <th>travel_&amp;_adventure</th>\n",
       "      <th>youth_&amp;_student_life</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>71</td>\n",
       "      <td>0.501142</td>\n",
       "      <td>0.275038</td>\n",
       "      <td>0.223821</td>\n",
       "      <td>57</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.144144</td>\n",
       "      <td>0.165130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.550684</td>\n",
       "      <td>0.032012</td>\n",
       "      <td>0.006912</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.076754</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-02</td>\n",
       "      <td>93</td>\n",
       "      <td>0.483342</td>\n",
       "      <td>0.388767</td>\n",
       "      <td>0.127891</td>\n",
       "      <td>71</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.478873</td>\n",
       "      <td>0.154978</td>\n",
       "      <td>0.185990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019952</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>0.007780</td>\n",
       "      <td>0.628945</td>\n",
       "      <td>0.023301</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0.058360</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.004201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-03</td>\n",
       "      <td>87</td>\n",
       "      <td>0.426233</td>\n",
       "      <td>0.373209</td>\n",
       "      <td>0.200558</td>\n",
       "      <td>78</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142020</td>\n",
       "      <td>0.235458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.016134</td>\n",
       "      <td>0.547873</td>\n",
       "      <td>0.033602</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.039875</td>\n",
       "      <td>0.077551</td>\n",
       "      <td>0.012950</td>\n",
       "      <td>0.003873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-04</td>\n",
       "      <td>71</td>\n",
       "      <td>0.523883</td>\n",
       "      <td>0.304259</td>\n",
       "      <td>0.171858</td>\n",
       "      <td>58</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.140071</td>\n",
       "      <td>0.260048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031340</td>\n",
       "      <td>0.018186</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>0.037701</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.042403</td>\n",
       "      <td>0.071947</td>\n",
       "      <td>0.003538</td>\n",
       "      <td>0.003296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-05</td>\n",
       "      <td>102</td>\n",
       "      <td>0.472078</td>\n",
       "      <td>0.317763</td>\n",
       "      <td>0.210159</td>\n",
       "      <td>77</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>0.120174</td>\n",
       "      <td>0.197293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014166</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.018849</td>\n",
       "      <td>0.449201</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.074956</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>0.005427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  tweet_count       neg       neu       pos  nlp_tweet_count  \\\n",
       "0  2025-01-01           71  0.501142  0.275038  0.223821               57   \n",
       "1  2025-01-02           93  0.483342  0.388767  0.127891               71   \n",
       "2  2025-01-03           87  0.426233  0.373209  0.200558               78   \n",
       "3  2025-01-04           71  0.523883  0.304259  0.171858               58   \n",
       "4  2025-01-05          102  0.472078  0.317763  0.210159               77   \n",
       "\n",
       "   not_polarized  polarized     anger   disgust  ...    gaming  \\\n",
       "0       0.368421   0.631579  0.144144  0.165130  ...  0.002792   \n",
       "1       0.521127   0.478873  0.154978  0.185990  ...  0.019952   \n",
       "2       0.500000   0.500000  0.142020  0.235458  ...  0.013754   \n",
       "3       0.396552   0.603448  0.140071  0.260048  ...  0.031340   \n",
       "4       0.441558   0.558442  0.120174  0.197293  ...  0.014166   \n",
       "\n",
       "   learning_&_educational     music  news_&_social_concern  other_hobbies  \\\n",
       "0                0.004627  0.016108               0.550684       0.032012   \n",
       "1                0.015618  0.007780               0.628945       0.023301   \n",
       "2                0.017788  0.016134               0.547873       0.033602   \n",
       "3                0.018186  0.008740               0.527992       0.037701   \n",
       "4                0.013848  0.018849               0.449201       0.032162   \n",
       "\n",
       "   relationships  science_&_technology    sports  travel_&_adventure  \\\n",
       "0       0.006912              0.004881  0.076754            0.005526   \n",
       "1       0.004411              0.004784  0.058360            0.003083   \n",
       "2       0.007094              0.039875  0.077551            0.012950   \n",
       "3       0.006327              0.042403  0.071947            0.003538   \n",
       "4       0.010302              0.007183  0.074956            0.006958   \n",
       "\n",
       "   youth_&_student_life  \n",
       "0              0.002157  \n",
       "1              0.004201  \n",
       "2              0.003873  \n",
       "3              0.003296  \n",
       "4              0.005427  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge with complete date, fill missing days with zero\n",
    "final_daily_df = final_daily_df.merge(tweet_counts_daily, on=\"date\", how=\"left\").fillna(0)\n",
    "final_daily_df[\"tweet_count\"] = final_daily_df[\"tweet_count\"].astype(int)\n",
    "final_daily_df = final_daily_df.merge(sentiment_daily, on=\"date\", how=\"left\")\n",
    "final_daily_df = final_daily_df.merge(emotion_daily, on=\"date\", how=\"left\")\n",
    "final_daily_df = final_daily_df.merge(personality_daily, on=\"date\", how=\"left\")\n",
    "final_daily_df = final_daily_df.merge(daily_word_counts, on=\"date\", how=\"left\")\n",
    "final_daily_df = final_daily_df.merge(topics_daily, on=\"date\", how=\"left\")\n",
    "\n",
    "display(final_daily_df.info())\n",
    "display(final_daily_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1109fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_daily_df.to_csv(os.path.join('processed', 'final_daily_df.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b13451",
   "metadata": {},
   "source": [
    "# Vizualization\n",
    "\n",
    "Visualize (daily and rolling window):\n",
    "- Tweet activity over time\n",
    "- Sentiment development over time\n",
    "- Polarisation percentage over time\n",
    "- Ekman Emotions over time\n",
    "- Big 5 Personality traits over time\n",
    "- Word distribution over time\n",
    "- Tweet topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "730402a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arguments should have the same length. The length of argument `y` is 2, whereas the length of previously-processed arguments ['date'] is 103",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 17\u001b[0m\n\u001b[0;32m      1\u001b[0m fig1 \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mline(\n\u001b[0;32m      2\u001b[0m     final_daily_df,\n\u001b[0;32m      3\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElon Musks Tweet-Aktivität über Zeit (interaktiv)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m fig2 \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mline(\n\u001b[0;32m     10\u001b[0m     final_daily_df,\n\u001b[0;32m     11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDurchschnittliches Sentiment pro Tag (interaktiv)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m fig3 \u001b[38;5;241m=\u001b[39m \u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_daily_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshare_polarized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshare_neutral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnteil\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvariable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKategorie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnteil polarisiert vs. neutral pro Tag (interaktiv)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m fig1\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     26\u001b[0m fig2\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\paull\\anaconda3\\envs\\VollVertnuettelt\\lib\\site-packages\\plotly\\express\\_chart_types.py:270\u001b[0m, in \u001b[0;36mline\u001b[1;34m(data_frame, x, y, line_group, color, line_dash, symbol, hover_name, hover_data, custom_data, text, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, error_x, error_x_minus, error_y, error_y_minus, animation_frame, animation_group, category_orders, labels, orientation, color_discrete_sequence, color_discrete_map, line_dash_sequence, line_dash_map, symbol_sequence, symbol_map, markers, log_x, log_y, range_x, range_y, line_shape, render_mode, title, subtitle, template, width, height)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mline\u001b[39m(\n\u001b[0;32m    222\u001b[0m     data_frame\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m go\u001b[38;5;241m.\u001b[39mFigure:\n\u001b[0;32m    266\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m    In a 2D line plot, each row of `data_frame` is represented as a vertex of\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    a polyline mark in 2D space.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScatter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paull\\anaconda3\\envs\\VollVertnuettelt\\lib\\site-packages\\plotly\\express\\_core.py:2483\u001b[0m, in \u001b[0;36mmake_figure\u001b[1;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[0;32m   2480\u001b[0m layout_patch \u001b[38;5;241m=\u001b[39m layout_patch \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m   2481\u001b[0m apply_default_cascade(args)\n\u001b[1;32m-> 2483\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m constructor \u001b[38;5;129;01min\u001b[39;00m [go\u001b[38;5;241m.\u001b[39mTreemap, go\u001b[38;5;241m.\u001b[39mSunburst, go\u001b[38;5;241m.\u001b[39mIcicle] \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2485\u001b[0m     args \u001b[38;5;241m=\u001b[39m process_dataframe_hierarchy(args)\n",
      "File \u001b[1;32mc:\\Users\\paull\\anaconda3\\envs\\VollVertnuettelt\\lib\\site-packages\\plotly\\express\\_core.py:1729\u001b[0m, in \u001b[0;36mbuild_dataframe\u001b[1;34m(args, constructor)\u001b[0m\n\u001b[0;32m   1726\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;66;03m# now that things have been prepped, we do the systematic rewriting of `args`\u001b[39;00m\n\u001b[1;32m-> 1729\u001b[0m df_output, wide_id_vars \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_args_into_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwide_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pd_like\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnative_namespace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1737\u001b[0m df_output: nw\u001b[38;5;241m.\u001b[39mDataFrame\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;66;03m# now that `df_output` exists and `args` contains only references, we complete\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# the special-case and wide-mode handling by further rewriting args and/or mutating\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;66;03m# df_output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paull\\anaconda3\\envs\\VollVertnuettelt\\lib\\site-packages\\plotly\\express\\_core.py:1375\u001b[0m, in \u001b[0;36mprocess_args_into_dataframe\u001b[1;34m(args, wide_mode, var_name, value_name, is_pd_like, native_namespace)\u001b[0m\n\u001b[0;32m   1372\u001b[0m         col_name \u001b[38;5;241m=\u001b[39m _check_name_not_reserved(field, reserved_names)\n\u001b[0;32m   1374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;129;01mand\u001b[39;00m (len_arg \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mlen\u001b[39m(argument)) \u001b[38;5;241m!=\u001b[39m length:\n\u001b[1;32m-> 1375\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1376\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arguments should have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1377\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of argument `\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m` is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, whereas the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1378\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength of previously-processed arguments \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1379\u001b[0m             \u001b[38;5;241m%\u001b[39m (field, len_arg, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mlist\u001b[39m(df_output\u001b[38;5;241m.\u001b[39mkeys())), length)\n\u001b[0;32m   1380\u001b[0m         )\n\u001b[0;32m   1382\u001b[0m     df_output[\u001b[38;5;28mstr\u001b[39m(col_name)] \u001b[38;5;241m=\u001b[39m to_named_series(\n\u001b[0;32m   1383\u001b[0m         x\u001b[38;5;241m=\u001b[39margument,\n\u001b[0;32m   1384\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(col_name),\n\u001b[0;32m   1385\u001b[0m         native_namespace\u001b[38;5;241m=\u001b[39mnative_namespace,\n\u001b[0;32m   1386\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;66;03m# Finally, update argument with column name now that column exists\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: All arguments should have the same length. The length of argument `y` is 2, whereas the length of previously-processed arguments ['date'] is 103"
     ]
    }
   ],
   "source": [
    "fig1 = px.line(\n",
    "    final_daily_df,\n",
    "    x=\"date\",\n",
    "    y=[\"tweet_count\", \"nlp_tweet_count\"],\n",
    "    labels={\"value\": \"Tweetanzahl\", \"variable\": \"Typ\"},\n",
    "    title=\"Elon Musks Tweet-Aktivität über Zeit (interaktiv)\"\n",
    ")\n",
    "\n",
    "fig2 = px.line(\n",
    "    final_daily_df,\n",
    "    x=\"date\",\n",
    "    y=[\"pos\", \"neu\", \"neg\"],\n",
    "    labels={\"value\": \"Sentiment-Wahrscheinlichkeit\", \"variable\": \"Sentiment\"},\n",
    "    title=\"Durchschnittliches Sentiment pro Tag (interaktiv)\"\n",
    ")\n",
    "\n",
    "fig3 = px.line(\n",
    "    final_daily_df,\n",
    "    x=\"date\",\n",
    "    y=[\"share_polarized\", \"share_neutral\"],\n",
    "    labels={\"value\": \"Anteil\", \"variable\": \"Kategorie\"},\n",
    "    title=\"Anteil polarisiert vs. neutral pro Tag (interaktiv)\"\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "\n",
    "window_size = 365\n",
    "\n",
    "rolling_sentiment = final_daily_df[['pos', 'neu', 'neg']].rolling(window=window_size, min_periods=1).mean()\n",
    "rolling_meta = final_daily_df[['tweet_count', 'share_polarized', 'share_neutral']].rolling(window=window_size, min_periods=1).mean()\n",
    "rolling_emotions = emotion_daily[['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']].rolling(window=window_size, min_periods=1).mean()\n",
    "rolling_emotions[\"date\"] = emotion_daily[\"date\"]\n",
    "rolling_personality = personality_daily[['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']].rolling(window=window_size, min_periods=1).mean()\n",
    "rolling_personality[\"date\"] = personality_daily[\"date\"]\n",
    "\n",
    "# Plot 1: Tweet-Aktivität\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=final_daily_df[\"date\"], y=rolling_meta['tweet_count'], mode='lines', name='Tweet Count (Rolling)'))\n",
    "fig1.add_vline(x=\"2022-10-27\", line=dict(color=\"red\", dash=\"dash\"), name=\"Twitter-Übernahme\")\n",
    "fig1.update_layout(title=\"Rolling Tweet Count\", xaxis_title=\"Datum\", yaxis_title=\"Tweetanzahl\")\n",
    "\n",
    "# Plot 2: Sentiment-Trends\n",
    "fig2 = go.Figure()\n",
    "for col in ['pos', 'neu', 'neg']:\n",
    "    fig2.add_trace(go.Scatter(x=final_daily_df[\"date\"], y=rolling_sentiment[col], mode='lines', name=col.capitalize()))\n",
    "fig2.add_vline(x=\"2022-10-27\", line=dict(color=\"red\", dash=\"dash\"))\n",
    "fig2.update_layout(title=\"Rolling Sentiment Trends\", xaxis_title=\"Datum\", yaxis_title=\"Anteil (0–1)\")\n",
    "\n",
    "# Plot 3: Polarisiert vs Neutral\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(x=final_daily_df[\"date\"], y=rolling_meta['share_polarized'], name='Polarisiert', mode='lines'))\n",
    "fig3.add_trace(go.Scatter(x=final_daily_df[\"date\"], y=rolling_meta['share_neutral'], name='Neutral', mode='lines'))\n",
    "fig3.update_layout(title=\"Rolling Anteil polarisiert vs. neutral\", xaxis_title=\"Datum\", yaxis_title=\"Anteil\")\n",
    "\n",
    "# Plot 4: Emotionen\n",
    "fig4 = go.Figure()\n",
    "for col in ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']:\n",
    "    fig4.add_trace(go.Scatter(x=rolling_emotions[\"date\"], y=rolling_emotions[col], mode='lines', name=col.capitalize()))\n",
    "fig4.update_layout(title=\"Rolling Emotion Scores\", xaxis_title=\"Datum\", yaxis_title=\"Score\")\n",
    "\n",
    "# Plot 5: Persönlichkeit\n",
    "fig5 = go.Figure()\n",
    "for col in ['Extroversion', 'Neuroticism', 'Agreeableness', 'Conscientiousness', 'Openness']:\n",
    "    fig5.add_trace(go.Scatter(x=rolling_personality[\"date\"], y=rolling_personality[col], mode='lines', name=col))\n",
    "fig5.update_layout(title=\"Rolling Big Five Traits\", xaxis_title=\"Datum\", yaxis_title=\"Score (0–1)\")\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "fig4.show()\n",
    "fig5.show()\n",
    "\n",
    "# vizulaize Top 50 words\n",
    "plt.figure(figsize=(14, 100))\n",
    "sns.barplot(data=word_counts.head(100), x=\"count\", y=\"word\", palette=\"viridis\")\n",
    "plt.title(\"Top 50 Words in Musk's Tweets\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Word\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VollVertnuettelt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
